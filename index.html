<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <title>Omry Sendik</title>
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 180px;
    height: 180px;
    position: relative;
    }
    .two
    {
    width: 180px;
    height: 180px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="favicon.ico">
  <title>Roey Mechrez</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Omry Sendik</name>
        </p>
        <p align="justify">
		    I am currently a Co-Founder and CTO in an InsurTech startup, where I lead a group of engineers.
For the past decade or so, I was affiliated with Samsung. In my last position, I was head of the automotive algorithms group (roughly 40 team mates) who worked on ISP, CV and ML algorithms.
I completed my PhD in the school of Computer Science of Tel-Aviv University, under the joint supervision of Prof. Daniel Cohen-Or and Prof. Dani Lischinski. My dissertation focused on Image Synthesis through weak supervision, by employing Neural Networks. I completed my MSc in the school of Electrical Engineering of Tel-Aviv University, where I was supervised by Prof. Hagit Messer Yaron. My thesis was in the sampling theory realm. Prior to that, I obtained a BSc in Electrical Engineering and a BA in Physics both at the Technion.
Finally, I am a proud (yet often absent) father.
        </p>

        <p align=center>
          <a href="mailto:omrysendik@gmail.com"><img style="width: 25px" src="Email.png"></a> &nbsp;  &nbsp;
          <a href="https://twitter.com/OmrySendik"> <img style="width: 25px" src="TwitterIcon.png"></a> &nbsp;  &nbsp;
          <!--<a href="RoeyMechrez-bio.txt"> Bio /</a>-->
          <a href="https://scholar.google.co.il/citations?user=HLtKby8AAAAJ&hl=en"> <img style="width: 25px" src="GoogleScholarIcon.jpg"></a> &nbsp;  &nbsp;
          <a href="https://www.linkedin.com/in/omrysendik/"> <img style="width: 25px" src="LinkedinIcon.png"></a> &nbsp;  &nbsp;
		  <a href="https://github.com/omrysendik"> <img style="width: 25px" src="GithubIcon.png">  </a> &nbsp;  &nbsp;
        </p>
        </td>
        <td width="33%">
        <img src="OS.jpeg" width="90%" height="90%">
        </td>

      </tr>
      </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
		<heading>News</heading>
		<p>
		6/3/20  - Presenting Unsupervised-k-modal-styled-content-generation in Berkeley
		<br>
		4/3/20  - Won the WACV Doctoral Travel Grant
		<br>
		2/3/20  - Presenting Unsupervised-k-modal-styled-content-generation in Stanford
	</p>
	<heading>Research</heading>
          <p>
          I'm interested in Computer Vision, Machine Learning, Generative Models and Image Processing
          </p>
        </td>
      </tr>
      </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>

    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>
	
    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>

    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>

    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>

    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>
	
    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>

    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>
	  
    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>

    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>
	
    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>

    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>
	  
	  
		
    </td>
    </tr>
  </table>
  </body>
</html>
