<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <title>Omry Sendik</title>
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 180px;
    height: 180px;
    position: relative;
    }
    .two
    {
    width: 180px;
    height: 180px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="favicon.ico">
  <title>Roey Mechrez</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Omry Sendik</name>
        </p>
        <p align="justify">
		    I am currently a Co-Founder and CTO in an InsurTech startup, where I lead a group of engineers.
For the past decade or so, I was affiliated with Samsung. In my last position, I was head of the automotive algorithms group (roughly 40 team mates) who worked on ISP, CV and ML algorithms.
I completed my PhD in the school of Computer Science of Tel-Aviv University, under the joint supervision of Prof. Daniel Cohen-Or and Prof. Dani Lischinski. My dissertation focused on Image Synthesis through weak supervision, by employing Neural Networks. I completed my MSc in the school of Electrical Engineering of Tel-Aviv University, where I was supervised by Prof. Hagit Messer Yaron. My thesis was in the sampling theory realm. Prior to that, I obtained a BSc in Electrical Engineering and a BA in Physics both at the Technion.
Finally, I am a proud (yet often absent) father.
        </p>

        <p align=center>
          <a href="mailto:omrysendik@gmail.com"><img style="width: 25px" src="Email.png"></a> &nbsp;  &nbsp;
          <a href="https://twitter.com/OmrySendik"> <img style="width: 25px" src="TwitterIcon.png"></a> &nbsp;  &nbsp;
          <!--<a href="RoeyMechrez-bio.txt"> Bio /</a>-->
          <a href="https://scholar.google.co.il/citations?user=HLtKby8AAAAJ&hl=en"> <img style="width: 25px" src="GoogleScholarIcon.jpg"></a> &nbsp;  &nbsp;
          <a href="https://www.linkedin.com/in/omrysendik/"> <img style="width: 25px" src="LinkedinIcon.png"></a> &nbsp;  &nbsp;
		  <a href="https://github.com/omrysendik"> <img style="width: 25px" src="GithubIcon.png">  </a> &nbsp;  &nbsp;
        </p>
        </td>
        <td width="33%">
        <img src="OS.jpeg" width="90%" height="90%">
        </td>

      </tr>
      </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
		<heading>News</heading>
		<p>
		6/3/20  - Presenting Unsupervised-k-modal-styled-content-generation in Berkeley
		<br>
		4/3/20  - Won the WACV Doctoral Travel Grant
		<br>
		2/3/20  - Presenting Unsupervised-k-modal-styled-content-generation in Stanford
	</p>
	<heading>Research</heading>
          <p>
          I'm interested in Computer Vision, Machine Learning, Generative Models and Image Processing
          </p>
        </td>
      </tr>
      </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <!--=================Paper UMMGAN ==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>

    <!--=================Paper DeepAge==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'DeepAge_image'><img style="width: 180px" src='DeepAge.png'></div>
        <img style="width: 180px" src='DeepAge.png'>
        </div>
        <script type="text/javascript">
        function DeepAge_start() {
        document.getElementById('DeepAge_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('DeepAge_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://www.dropbox.com/s/vhepdd27f730k8r/deep_age_paper_twocol_v5.pdf?dl=0">
        <papertitle>DeepAge: Deep Learning of face-based age estimation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Yossi Keller
		  <br>
		  <em>Signal Processing: Image Communication 78 (2019)</em> <br>
        <br>
        <p></p>
        <p>We present a dual Convolutional Neural Network (CNN) and Support Vector Regression (SVR) approach for face-based age estimation. A CNN is trained for representation learning, followed by Metric Learning, after which SVR is applied to the learned features. This allows to overcome the lack of large datasets with age annotations, by initially training the CNN for face recognition.</p>
      </td>
    </tr>
 
	  
    <!--=================Paper CrossNet==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'CrossNet_image'><img style="width: 180px" src='CrossNet.png'></div>
        <img style="width: 180px" src='CrossNet.png'>
        </div>
        <script type="text/javascript">
        function CrossNet_start() {
        document.getElementById('CrossNet_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('CrossNet_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Sendik_CrossNet_Latent_Cross-Consistency_for_Unpaired_Image_Translation_WACV_2020_paper.pdf">
        <papertitle>CrossNet: Latent Cross-Consistency for Unpaired Image Translation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>The IEEE Winter Conference on Applications of Computer Vision 2019 (WACV)</em> <br>
        <br>
        <p></p>
        <p>We introduce a novel architecture for un-paired image translation, and explore several new regularizers enabled by it. Specifically, our architecture comprises a pair of GANs, as well as a pair of translators between their respective latent spaces. These cross-translators enable us to impose several regularizing constraints on the learnt image translation operator, collectively referred to as latent cross-consistency.</p>
      </td>
    </tr>

    <!--=================Paper IMNet==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'IMNet_image'><img style="width: 180px" src='IMNet.png'></div>
        <img style="width: 180px" src='IMNet.png'>
        </div>
        <script type="text/javascript">
        function IMNet_start() {
        document.getElementById('IMNet_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('IMNet_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://www.dropbox.com/s/3nen3v68xve9h5b/IM_Net_paper_submission_CVPR19.pdf?dl=0">
        <papertitle>IM-Net for High Resolution Video Frame Interpolation</papertitle></a><br>
          Tomer Peleg, Pablo Szekely, Doron Sabo and <strong>Omry Sendik</strong>
		  <br>
		  <em>CVPR 2019</em> <br>
        <br>
        <p></p>
        <p>In this paper we propose IM-Net: an interpolated motion neural network. We used an economic structured architecture and end-to-end training with multi-scale tailored losses. In particular, we formulate interpolated motion estimation as classification rather than regression. IM-Net out-performs previous methods by more than 1.3dB (PSNR) ona high resolution version of the recently introduced Vimeo triplet dataset. Moreover, the network runs in less than 33msec on a single GPU for HD resolution.</p>
      </td>
    </tr>
	  
    <!--=================Paper WhatsInAFace==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'WhatsInAFace_image'><img style="width: 180px" src='WhatsInAFace.png'></div>
        <img style="width: 180px" src='WhatsInAFace.png'>
        </div>
        <script type="text/javascript">
        function WhatsInAFace_start() {
        document.getElementById('WhatsInAFace_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('WhatsInAFace_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://www.dropbox.com/s/3fb1jdgnpb7jjy9/EG19_Whats_in_a_Face.pdf?dl=0">
        <papertitle>What's in a Face? Metric Learning for Face Characterization</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>Euro Graphics 2019</em> <br>
        <br>
        <p></p>
        <p>We present a method for determining which facial parts (mouth, nose, etc.) best characterize an individual, given a set of that individual’s portraits. We introduce a novel distinctiveness analysis of a set of portraits, which leverages the deep features extracted by a pre-trained face recognition CNN and a hair segmentation FCN, in the context of a weakly supervised metric learning scheme. Our analysis enables the generation of a polarized class activation map (PCAM) for an individual’s portrait via a transformation that localizes and amplifies the discriminative regions of the deep feature maps extracted by the aforementioned networks. A user study that we conducted shows that there is a surprisingly good agreement between the face parts that users indicate as characteristic and the face parts automatically selected by our method. We demonstrate a few applications of our method, including determining the most and the least representative portraits among a set of portraits of an individual, and the creation of facial hybrids: portraits that combine the characteristic recognizable facial features of two individuals. Our face characterization analysis is also effective for ranking portraits in order to find an individual’s look-alikes (Doppelgängers).</p>
      </td>
    </tr>

    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>
	
    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>

    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>
	  
    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>

    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>
	
    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>

    <!--=================Paper==========================-->
    <tr onmouseout="TTT_stop()" onmouseover="ttt_start()" bgcolor="#ffffd0">
      <td width="25%">
        <div class="one">
        <div  class="two" id = 'UMMGAN_image'><img style="width: 180px" src='UMMGAN.png'></div>
        <img style="width: 180px" src='UMMGAN.png'>
        </div>
        <script type="text/javascript">
        function UMMGAN_start() {
        document.getElementById('UMMGAN_image').style.opacity = "1";
        }
        function ttt_stop() {
        document.getElementById('UMMGAN_image').style.opacity = "0";
        }
        ttt_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/2001.03640">
        <papertitle>Unsupervised K-Modal Styled Content Generation</papertitle></a><br>
          <strong>Omry Sendik</strong>, Dani Lischinski and Daniel Cohen-Or
		  <br>
		  <em>ACM Transactions on Graphics 2020</em> <br>
        <br>
        <p></p>
        <p>We introduce uMM-GAN, a novel architecture designed to better model multi-modal distributions, in an unsupervised fashion. Building upon the StyleGAN architecture, our network learns multiple modes, in a completely unsupervised manner, and combines them using a set of learned weights. We demonstrate that this approach is capable of effectively approximating a complex distribution as a superposition of multiple simple ones. We further show that uMM-GAN effectively disentangles between modes and style, thereby providing an independent degree of control over the generated content.</p>
      </td>
    </tr>
	  
	  
		
    </td>
    </tr>
  </table>
  </body>
</html>
